{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UD English corpora analysis. \n",
    "## Objectives\n",
    "\n",
    "1. For each dependency relation in UD, identify every single occurence of it, in every available English UD corpora. \n",
    "2. Obtain minimal information on each of these occurence (basic UDdeprel, Head and Dep classes, a helper function to \n",
    "to iterate through the list of deprels to extract info on). \n",
    "2a. do we care about the sub tags such nsubj:pass? Yes, it helps us identify possible subcases to analyse. \n",
    "3. Aggregate the data for these occurences at the dependency relation level to obtain insights (pairplots, correlation+heatmap for each POS-TAG pair)\n",
    "4. Where necessary, expand the analysis for a particular dependency relation (e.g. look at all Dep nodes and/or grandparent nodes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Useful notes identified along the process__\n",
    "1. inconsistency in data captured in each UD English corpora. For instance, the ESL corpora does not capture lemma information. \n",
    "2. slight variations in relations numbering. EWT uses sub-decimals, \n",
    "3. ESL captures contractions, e.g. \"cannot\" is expanded into \"can\" and \"not\", and the dependency relations are tagged accordingly. \"cannot\" is captured too, but given a token index that is a combination of its units, e.g. 10-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu # https://github.com/EmilStenstrom/conllu\n",
    "import pandas as pd\n",
    "import glob\n",
    "from conllu import parse\n",
    "from conllu import parse_incr\n",
    "import re\n",
    "import _pickle as pickle\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Consolidating the datasets, extracting only the relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all the filenames for the English UD corporas \n",
    "conllu_filenames = glob.glob(\"./UD_eng_rawfiles/*.conllu\")\n",
    "conllu_filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  ./UD_eng_rawfiles/en_cesl-ud-dev.conllu  | total number of sentences:  500\n",
      "Processed:  ./UD_eng_rawfiles/en_cesl-ud-test.conllu  | total number of sentences:  1000\n",
      "Processed:  ./UD_eng_rawfiles/en_cesl-ud-train.conllu  | total number of sentences:  5124\n",
      "Processed:  ./UD_eng_rawfiles/en_ewt-ud-dev.conllu  | total number of sentences:  7126\n",
      "Processed:  ./UD_eng_rawfiles/en_ewt-ud-test.conllu  | total number of sentences:  9203\n",
      "Processed:  ./UD_eng_rawfiles/en_ewt-ud-train.conllu  | total number of sentences:  21746\n",
      "Processed:  ./UD_eng_rawfiles/en_gum-ud-dev.conllu  | total number of sentences:  22453\n",
      "Processed:  ./UD_eng_rawfiles/en_gum-ud-test.conllu  | total number of sentences:  23231\n",
      "Processed:  ./UD_eng_rawfiles/en_gum-ud-train.conllu  | total number of sentences:  26145\n",
      "Processed:  ./UD_eng_rawfiles/en_lines-ud-dev.conllu  | total number of sentences:  27057\n",
      "Processed:  ./UD_eng_rawfiles/en_lines-ud-test.conllu  | total number of sentences:  27971\n",
      "Processed:  ./UD_eng_rawfiles/en_lines-ud-train.conllu  | total number of sentences:  30709\n",
      "Processed:  ./UD_eng_rawfiles/en_partut-ud-dev.conllu  | total number of sentences:  30865\n",
      "Processed:  ./UD_eng_rawfiles/en_partut-ud-test.conllu  | total number of sentences:  31018\n",
      "Processed:  ./UD_eng_rawfiles/en_partut-ud-train.conllu  | total number of sentences:  32799\n",
      "Processed:  ./UD_eng_rawfiles/en_pud-ud-test.conllu  | total number of sentences:  33799\n"
     ]
    }
   ],
   "source": [
    "def conllu_metadata_stripper(filename:str, sentence_dict:dict): \n",
    "    '''\n",
    "    takes a file containing text in CONLLU format, filters for tokens (removing metadata), collects them into sentences, \n",
    "    adds them to the dictionary of sentences. \n",
    "    inputs | filename:str - name of the file to be processed ; sentence_dict:dict - dictionary, either empty\n",
    "    or containing CONLLU tokens in sentences (such as those already processed from other files with CONLLU data) \n",
    "    returns | sentence_dict:dict - containing the tokens collected from the CONLLU file being processed\n",
    "    '''\n",
    "    #open the file \n",
    "    data_file = open(filename,\"r\",encoding=\"utf-8\")\n",
    "    # get all the sentences... i.e. by collecting all the tokens and ignoring the metadata before every sentence\n",
    "    token_list = [token for token in data_file.readlines() if re.match(r\"([0-9]+\\.?[0-9]*\\s)\\S*\",  token)] \n",
    "    # close the file \n",
    "    data_file.close()\n",
    "    \n",
    "\n",
    "    # use a counter identify each sentence added to the dictionary\n",
    "    sent_counter = len(sentence_dict)+1\n",
    "    \n",
    "    \n",
    "    def token_adder(sent_counter,sentence_dict):\n",
    "        # use a counter which we will control the addition of tokens to each key in the dictionary\n",
    "        tok_counter = 0\n",
    "        \n",
    "        # initialise an empty list with the current dictionary key \n",
    "        sentence_dict[sent_counter] = []\n",
    "        \n",
    "        # try except to pass when list has been emptied\n",
    "        try: \n",
    "            # \"local\" while loop to add tokens only if they have numbers bigger than tok_counter\n",
    "            # i.e. once the index number for a token falls below the last, reset the while loop\n",
    "            while float(re.findall(r\"^[0-9]+\\.?[0-9]*\", token_list[0])[0]) > tok_counter:\n",
    "                tok_counter = float(re.findall(r\"^[0-9]+\\.?[0-9]*\", token_list[0])[0])\n",
    "                \n",
    "                # pop the first element from the token_list \n",
    "                sentence_dict[sent_counter].append(token_list.pop(0))\n",
    "                 \n",
    "                \n",
    "                # note on the regex above: \n",
    "                # there is corpora that index tokens with floating point numbers. e.g. a token can be 18\n",
    "                # and another related one can be 18.1. see sentence 17838 in the dict, which is from the \n",
    "                # EWT corpora. The regex captures either the full string containing a float, or the full string\n",
    "                # containing an int. The string is then converted to a float (instead of int) to avoid errors \n",
    "                # in token identification due to roundings.\n",
    "                \n",
    "            else: \n",
    "                tok_counter = 0\n",
    "                pass\n",
    "        except: \n",
    "            pass \n",
    "        \n",
    "        \n",
    "    # while loop to continue calling token_adder recursively until the list is empty     \n",
    "    while len(token_list)>0: \n",
    "        token_adder(sent_counter,sentence_dict)\n",
    "        sent_counter += 1\n",
    "\n",
    "\n",
    "    return sentence_dict\n",
    "\n",
    "UDen_sent_dict = {}\n",
    "for file in conllu_filenames: \n",
    "    \n",
    "    conllu_metadata_stripper(file, UDen_sent_dict)\n",
    "    print (\"Processed: \", file, \" | total number of sentences: \", len(UDen_sent_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the parse_incr method from the conllu package to parse the information for each token\n",
    "# the parsing results in the creation of a TokenList object which contains each of the token in \n",
    "# a sentence and seperates the conllu data into seperate keys\n",
    "# to access a specific token, <dictionary>[<sentence number>][0][<token number>]\n",
    "\n",
    "UDen_sent_parsedlist = {}\n",
    "for sentence in UDen_sent_dict:\n",
    "    UDen_sent_parsedlist[sentence] = []\n",
    "    for token in parse_incr(UDen_sent_dict[sentence]):\n",
    "        UDen_sent_parsedlist[sentence].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Picklers in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load pickle file \n",
    "def pickleloader(filename):\n",
    "    # # open the file for writing\n",
    "    fileObject = open(filename,'rb') \n",
    "\n",
    "    # load the object from the file into var univ_processed_train\n",
    "    return pickle.load(fileObject,  encoding=\"latin1\")  \n",
    "    #latin1 here, to bypass python2 to 3 pickle problem\n",
    "\n",
    "    # here we close the fileObject\n",
    "    fileObject.close()\n",
    "    \n",
    "# function to create a file and store the data in the file \n",
    "def picklemaker(filename, objectname): \n",
    "    # open the file for writing\n",
    "    fileObject = open(filename,'wb') \n",
    "\n",
    "    # this writes the object a to the file named 'testfile'\n",
    "    pickle.dump(objectname,fileObject)   \n",
    "\n",
    "    # here we close the fileObject\n",
    "    fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "picklemaker(\"./UDdeprel_data/UDen_sent_parsedlist\", UDen_sent_parsedlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. UDdeprel classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UDdeprel:\n",
    "    '''\n",
    "    basic class of objects for capturing tokens with a particular UD dependency relationship, its form and POS tag. \n",
    "    This is the dependent node on which the deprel lands on. The class also includes functions as well as functions \n",
    "    to find its head and dependents. \n",
    "    --  -- \n",
    "    Note that self.head is a UDdeprel object. \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, sentence_num:int, self_id:int, self_form:str, deprel:str, \n",
    "                 self_pos:str, head_id: int, deprel_sub=None, head=None, \n",
    "                 deps=None, siblings=None):\n",
    "        self.form = self_form             # using form instead of lemma because some UD Eng corpora (e.g. CESL) don't have lemma information \n",
    "        self.self_id = self_id\n",
    "        self.deprel = deprel\n",
    "        self.deprel_sub = deprel_sub\n",
    "        self.self_pos = self_pos\n",
    "        self.sentence_num = sentence_num\n",
    "        self.head_id = head_id\n",
    "        self.head = head\n",
    "        self.deps = deps\n",
    "        self.siblings = siblings\n",
    "    \n",
    "    def __str__(self):\n",
    "        print(\"Form:\", self.form, \" DepRel:\", self.deprel, \" DepRel_Sub:\", self.deprel_sub,\n",
    "             \" PoS:\", self.self_pos, \" SentNum:\", self.sentence_num)\n",
    "    \n",
    "    def find_head(self, UD_sent_dictionary):\n",
    "        sentence = UD_sent_dictionary[self.sentence_num][0] # [0] is required by default for every sentence to access its tokesn\n",
    "       \n",
    "        H_sentence_num = self.sentence_num\n",
    "        H_self_id = self.head_id                        # this is the id number in the conllu format \n",
    "                                                        # and not the same as the dictionary's\n",
    "                                                        # which is indexed from 0\n",
    "                \n",
    "        # find the token having the H_self_id in its \"id\" key\n",
    "        # this approach ensures accuracy, since token id numbers are not aligned \n",
    "        # with index numbers in sentences (e.g. there are no root tokens), certain \n",
    "        # UD English corpora (e.g. EWT) use sub index numbers (e.g. 11 and 11.1) for \n",
    "        # labeling the tokens in the corpus. \n",
    "        H_token =  [token for token in sentence if token[\"id\"]==H_self_id][0]\n",
    "        \n",
    "        H_self_form = H_token[\"form\"]\n",
    "        H_head_id = H_token[\"head\"]\n",
    "        H_deprel = re.findall(r\"^[A-Za-z]+[^:]\", H_token[\"deprel\"])[0]  \n",
    "        H_deprel_sub = H_token[\"deprel\"].lstrip(H_deprel+\":\")\n",
    "        if H_deprel_sub == \"\":\n",
    "            H_deprel_sub = None\n",
    "        H_self_pos = H_token[\"upostag\"]\n",
    "\n",
    "        __head = Head(sentence_num = H_sentence_num,\n",
    "                        self_id = H_self_id, \n",
    "                        self_form = H_self_form,\n",
    "                        head_id = H_head_id,\n",
    "                        deprel = H_deprel,\n",
    "                        self_pos = H_self_pos,\n",
    "                        deprel_sub = H_deprel_sub)\n",
    "                        \n",
    "                        # to prevent confusion, self_id = H_self_id+1 is required to return the token ID number to \n",
    "                        # one in the conllu data\n",
    "        self.head = __head\n",
    "        \n",
    "    def find_deps(self,UD_sent_dictionary):\n",
    "        dep_id = self.self_id # this is the index number of the primary UDdeprel token\n",
    "        sentence = UD_sent_dictionary[self.sentence_num][0] # [0] is required by default for every sentence to access its tokesn\n",
    "        \n",
    "        D_sentence_num = self.sentence_num\n",
    "        \n",
    "        D_tokens = [token for token in sentence if token[\"head\"]==dep_id]\n",
    "        # the conllu has a field to capture all the dependents of a particular token. \n",
    "        # however, not all UD Eng corpora (e.g. CESL) populate this field. Therefore, \n",
    "        # a seach of each token in a sentence has to be done here. \n",
    "        \n",
    "\n",
    "        \n",
    "        __deps = []\n",
    "        for D_token in D_tokens:\n",
    "            D_self_id = D_token[\"id\"]      \n",
    "            D_head_id = D_token[\"head\"]    # again, this is the index given in the conllu format\n",
    "            D_self_form = D_token[\"form\"],\n",
    "            D_head_id = D_token[\"head\"],\n",
    "            D_deprel = re.findall(r\"^[A-Za-z]+[^:]\", D_token[\"deprel\"])[0]\n",
    "            D_deprel_sub = D_token[\"deprel\"].lstrip(D_deprel+\":\")\n",
    "            if D_deprel_sub == \"\":\n",
    "                D_deprel_sub = None\n",
    "            D_self_pos = D_token[\"upostag\"]\n",
    "\n",
    "            # r\"^[A-Za-z]+[^:]\" for D_self_pos because some of the UD English corpora have the postag in all lowercase\n",
    "            # and some in all uppercase. \n",
    "\n",
    "\n",
    "            __dep = Dep(sentence_num = D_sentence_num,\n",
    "                            self_id = D_self_id,\n",
    "                            self_form = D_self_form,\n",
    "                            head_id = D_head_id,\n",
    "                            deprel = D_deprel,\n",
    "                            self_pos = D_self_pos,\n",
    "                            deprel_sub = D_deprel_sub)\n",
    "\n",
    "\n",
    "            __deps.append(__dep)\n",
    "        self.deps = __deps\n",
    "            \n",
    "    def find_siblings(self, UD_sent_dictionary):\n",
    "        head_id = self.head_id # conllu number of the head of the primary UDdeprel token\n",
    "        sentence = UD_sent_dictionary[self.sentence_num][0] # [0] is required by default for every sentence to access its tokesn\n",
    "        S_sentence_num = self.sentence_num\n",
    "        \n",
    "        # match only the tokens having the head_id, but exclude the one having self.self_id, to find only siblings\n",
    "        S_tokens = [token for token in sentence if token[\"head\"]==head_id and token[\"id\"] != self.self_id]\n",
    "        \n",
    "        __siblings = []\n",
    "        for S_token in S_tokens:\n",
    "            S_self_id = S_token[\"id\"]     \n",
    "            S_head_id = S_token[\"head\"]    # again, this is the index given in the conllu format\n",
    "            S_self_form = S_token[\"form\"],\n",
    "            S_head_id = S_token[\"head\"],\n",
    "            S_deprel = re.findall(r\"^[A-Za-z]+[^:]\", S_token[\"deprel\"])[0]\n",
    "            S_deprel_sub = S_token[\"deprel\"].lstrip(S_deprel+\":\")\n",
    "            if S_deprel_sub == \"\":\n",
    "                S_deprel_sub = None\n",
    "            S_self_pos = S_token[\"upostag\"]\n",
    "\n",
    "            __sibling = Sibling(sentence_num = S_sentence_num,\n",
    "                            self_id = S_self_id,\n",
    "                            self_form = S_self_form,\n",
    "                            head_id = S_head_id,\n",
    "                            deprel = S_deprel,\n",
    "                            self_pos = S_self_pos,\n",
    "                            deprel_sub = S_deprel_sub)\n",
    "\n",
    "\n",
    "            __siblings.append(__sibling)\n",
    "        self.siblings = __siblings\n",
    "\n",
    "class Head(UDdeprel):\n",
    "    '''\n",
    "    basic UDdeprel that inherits all the methods and attributes of the UDdeprel. \n",
    "    Amend or add to for the purpose of your analysis. \n",
    "    '''\n",
    "    pass\n",
    "    \n",
    "class Dep(UDdeprel): \n",
    "    '''\n",
    "    basic UDdeprel that inherits all the methods and attributes of the UDdeprel. \n",
    "    Amend or add to for the purpose of your analysis. \n",
    "    '''\n",
    "    pass\n",
    "\n",
    "class Sibling(UDdeprel):\n",
    "    '''\n",
    "    basic UDdeprel that inherits all the methods and attributes of the UDdeprel. \n",
    "    Intended for collecting information about a particular token's sibling tokens (i.e. \n",
    "    sharing the same Head node). Amend or add to for the purpose of your analysis. \n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UD_deprel_list = [\"nsubj\", \"obj\", \"iobj\", \"csubj\", \"ccomp\", \"xcomp\", \"obl\", \n",
    "                  \"vocative\", \"expl\",\"dislocated\",\"advcl\",\"advmod\", \"discourse\",\n",
    "                  \"aux\",\"cop\", \"mark\",\"nmod\",\"appos\", \"nummod\", \"acl\", \"amod\", \"det\", \n",
    "                  \"clf\", \"case\", \"conj\", \"cc\", \"fixed\", \"flat\", \"compound\", \"list\",\n",
    "                  \"parataxis\", \"orphan\", \"goeswith\", \"reparandum\", \"punct\", \"dep\"]\n",
    "\n",
    "# note: we are not including the root dependency relation in our analysis here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Function to collect basic information about tokens with a particular UD relation landing on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprelfinder(deprel, UD_sent_dictionary):\n",
    "    '''\n",
    "    takes a dictionary containing the sentences (and tokens) of one or more UD english corpora, searches for \n",
    "    tokens that have this dependency relation (these tokens would be the dependent nodes in a dep tree).  \n",
    "    This function aggregates data in a manner that would allow a more fine-grained analysis. A token's dependency\n",
    "    relation is set at main tag level, with subtag information captured at the deprel_sub level.  \n",
    "    _________________\n",
    "   \n",
    "    input | deprel:str, the dependency relation of interest; UD_sent_dictionary: dict, a dictionary containing \n",
    "    UD sentences as parsed from the conllu format with the conllu python package \n",
    "    (see https://github.com/EmilStenstrom/conllu); output: str, \"pickle\" by default, or \"csv\", the format to \n",
    "    save the dependency relation search results in. \n",
    "    \n",
    "    output | a pickle containing a list of UDdeprel objects containing information about all the tokens in the \n",
    "    UD_sent_dictionary, having the dependency relation of interest. The information captured by this function is \n",
    "    sentence_num, self_form, deprel, deprel_sub, self_pos. \n",
    "    '''\n",
    "    failures= [] \n",
    "    \n",
    "    __UDdeprel_list = []\n",
    "    \n",
    "    # loop through every sentence in the dictionary\n",
    "    for sentence in UD_sent_dictionary: #sentence is an int from 1 to n, where n is the size of the dictionary. \n",
    "        \n",
    "        # go into every token for each sentence and check for tokens that have the deprel we are interested in. \n",
    "        for token in UD_sent_dictionary[sentence][0]:\n",
    "            \n",
    "            #use regex to match deprels because there are some that have subtags such as nsubj:pass\n",
    "            if re.match(\"^\"+deprel, token[\"deprel\"]):\n",
    "                \n",
    "                sentence_num=sentence\n",
    "                self_id = token[\"id\"]   # this is the id number in the conllu format and not the same as the dictionary's\n",
    "                                        # which is indexed from 0\n",
    "                self_form=token[\"form\"]\n",
    "                head_id = token[\"head\"]\n",
    "                deprel = re.findall(r\"^[A-Za-z]+[^:]\", deprel)[0]  \n",
    "                deprel_sub = token[\"deprel\"].lstrip(deprel+\":\") #use lstrip to remove the main deprel+: tag\n",
    "                \n",
    "                if deprel_sub == \"\":\n",
    "                    deprel_sub=None\n",
    "                self_pos=token[\"upostag\"]\n",
    "                \n",
    "                __UDdeprel = UDdeprel(sentence_num = sentence_num,\n",
    "                                      self_id = self_id,\n",
    "                                      self_form = self_form, \n",
    "                                      head_id = head_id,\n",
    "                                      deprel = deprel, \n",
    "                                      self_pos = self_pos,\n",
    "                                      deprel_sub = deprel_sub)\n",
    "                \n",
    "                try:\n",
    "                    __UDdeprel.find_head(UDen_sent_parsedlist)\n",
    "                    __UDdeprel.find_deps(UDen_sent_parsedlist)\n",
    "                    __UDdeprel.find_siblings(UDen_sent_parsedlist)\n",
    "                except: \n",
    "                    failures.append(__UDdeprel)\n",
    "\n",
    "                __UDdeprel_list.append(__UDdeprel)\n",
    "    \n",
    "    # let's pickle the results\n",
    "    filename = \"./UDdeprel_data/\"+deprel+\"_UDdeprel_pkl\"\n",
    "    picklemaker(filename, __UDdeprel_list)\n",
    "    \n",
    "    return failures\n",
    "\n",
    "\n",
    "# run the function and start collecting the token info and storing into pickles. if a pickle file with the same \n",
    "# filename already exists, the picklemaker function opens the file and dumps the object in it. it overwrites the \n",
    "# existing content. if you want to save the version of the previous pickle file, adjust the filename variable in the \n",
    "# function. \n",
    "failures__ = []\n",
    "for deprel in UD_deprel_list: \n",
    "    failures__.extend(deprelfinder(deprel, UDen_sent_parsedlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# failure check on find_head and find_deps methods. \n",
    "len(failures__)\n",
    "# no failures in acquiring information. next step is some basic unit tests to check accuracy of acquired information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes \n",
    "1. Every UD dependency relation has a pickle file now. The start of each pickle filename identifies the dependency relation tag that it is for. \n",
    "2. In each pickle file is the list of tokens having that particular dependency relation. The tokens are stored as objects of the UDdeprel class from above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analysis and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_statistics(UDdeprel_list):\n",
    "    '''\n",
    "    Purpose of this function is to provide basic statistics on the phenomonena in one (or more) Universal \n",
    "    Dependencies-annotated corpora.\n",
    "    \n",
    "    returns | basic_stats which provides information about the set of UDdeprel objects for a particular UD dependency \n",
    "    relation tag. \n",
    "    \n",
    "    1a. head_dep_pos    | identifies the prototypical head-dep POS pair. returns a table with the counts and percentage \n",
    "                            of a particular POS-pair for the dependency relation tag of interest. \n",
    "    1b. deprel_subtags  | identifies the presence (or absence) of subtags in the corpora for the dependency relation tag. \n",
    "                            returns a table with count and percentage figures of the subtags. \n",
    "    2.  dep_subdep_pos  | identifies the prototypical dep-subdep POS pair and their dependency relation. returns a table \n",
    "                            with the counts and percentage of the occurrence across the corpora. \n",
    "    3a. deprel_on_head     | identifies the prototypical dependency relation that lands on the head. \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # =+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+=    \n",
    "    # 1a. governor-dependent POS analysis \n",
    "    __head_dep_pos = []\n",
    "    for dependent in UDdeprel_list: \n",
    "        head_POS = dependent.head.self_pos\n",
    "        dep_POS = dependent.self_pos\n",
    "        dep_deprel = dependent.deprel\n",
    "        __head_dep_pos.append(head_POS + \"_{}_\".format(dep_deprel) + dep_POS)\n",
    "    \n",
    "    __head_dep_pos_counter = collections.Counter(x for x in __head_dep_pos if x)\n",
    "    \n",
    "    df_1a = pd.DataFrame.from_dict(__head_dep_pos_counter, orient='index').reset_index()\n",
    "    df_1a.columns = [\"head_dep_pos\", \"count\"]\n",
    "    df_1a.sort_values(by=\"count\", ascending=False, inplace=True)\n",
    "    df_1a.reset_index(drop=True,inplace=True)\n",
    "    df_1a[\"percentage\"] = df_1a[\"count\"].copy()\n",
    "    df_1a[\"percentage\"] =  df_1a[\"percentage\"] / df_1a[\"count\"].sum()*100\n",
    "    \n",
    "    basic_stats = {\"head_dep_pos\": df_1a}\n",
    "    \n",
    "    # =+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+=\n",
    "    # 1b. presence of subtags, and frequency + percentage \n",
    "    \n",
    "    __deprel_subtags = [\"none\" if dependent.deprel_sub==None \n",
    "                        else dependent.deprel_sub for dependent in UDdeprel_list]\n",
    "    \n",
    "    __deprel_subtags_counter = collections.Counter(x for x in __deprel_subtags if x)\n",
    "\n",
    "    df_1b = pd.DataFrame.from_dict(__deprel_subtags_counter, orient='index').reset_index()\n",
    "    df_1b.columns = [\"deprel_subtag\", \"count\"]\n",
    "    df_1b.sort_values(by=\"count\", ascending=False, inplace=True)\n",
    "    df_1b.reset_index(drop=True,inplace=True)\n",
    "    df_1b[\"percentage\"] = df_1b[\"count\"].copy()\n",
    "    df_1b[\"percentage\"] =  df_1b[\"percentage\"] / df_1b[\"count\"].sum()*100   \n",
    "    \n",
    "    basic_stats[\"deprel_subtags\"] = df_1b\n",
    "\n",
    "    # =+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+=\n",
    "    # 2. Dependent analysis: gives the most prototypical dependent-subdependent relationship (i.e. their POS \n",
    "    # and deprel). \n",
    "    \n",
    "    __dep_subdep_pos = []\n",
    "    for dependent in UDdeprel_list:\n",
    "        dep_pos = dependent.self_pos\n",
    "        __subdeps = dependent.deps\n",
    "        for __subdep in __subdeps:\n",
    "            subdep_pos = __subdep.self_pos\n",
    "            subdep_deprel = __subdep.deprel\n",
    "            __dep_subdep_pos.append(dep_pos + \"_{}_\".format(subdep_deprel) + subdep_pos)\n",
    "    \n",
    "    __dep_subdep_pos_counter = collections.Counter(x for x in __dep_subdep_pos if x)\n",
    "\n",
    "    df_2 = pd.DataFrame.from_dict(__dep_subdep_pos_counter, orient='index').reset_index()\n",
    "    df_2.columns = [\"deprel_subtag\", \"count\"]\n",
    "    df_2.sort_values(by=\"count\", ascending=False, inplace=True)\n",
    "    df_2.reset_index(drop=True,inplace=True)\n",
    "    df_2[\"percentage\"] = df_2[\"count\"].copy()\n",
    "    df_2[\"percentage\"] =  df_2[\"percentage\"] / df_2[\"count\"].sum()*100   \n",
    "    \n",
    "    basic_stats[\"dep_subdep_pos\"] = df_2\n",
    "    \n",
    "    # =+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+=\n",
    "    # 3a. Head analysis: most common deprel main tag that lands on the head of the UDdeprel\n",
    "    \n",
    "    __deprel_on_head = [\"none\" if dependent.head.deprel==None \n",
    "                        else dependent.head.deprel for dependent in UDdeprel_list]\n",
    "    \n",
    "    __deprel_on_head_counter = collections.Counter(x for x in __deprel_on_head if x)\n",
    "\n",
    "    df_3a = pd.DataFrame.from_dict(__deprel_on_head_counter, orient='index').reset_index()\n",
    "    df_3a.columns = [\"deprel_on_head\", \"count\"]\n",
    "    df_3a.sort_values(by=\"count\", ascending=False, inplace=True)\n",
    "    df_3a.reset_index(drop=True,inplace=True)\n",
    "    df_3a[\"percentage\"] = df_3a[\"count\"].copy()\n",
    "    df_3a[\"percentage\"] =  df_3a[\"percentage\"]/df_3a[\"count\"].sum()*100   \n",
    "    \n",
    "    basic_stats[\"deprel_on_head\"] = df_3a\n",
    "    \n",
    "    # =+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+=\n",
    "    # 3b. Head analysis: most common dep_rels that exits the head of the UDdeprel, (ii) most common sets of \n",
    "    # dep_rels that exit the head, (iii) most common subsets of dep_rels that exit the head [using intersection]... \n",
    "    # filter off heads with single dep_rels [already inferable from (i)]\n",
    "    \n",
    "        # not clear if relevant yet. \n",
    "\n",
    "    # =+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+=\n",
    "    # 5. heatmap of the head data and the dependent data. e.g. identify correlation across the dataset.\n",
    "    \n",
    "        # work-in-progress\n",
    "    print(\"These results are for the __ {} __ dependency relation tag.\".format(dependent.deprel))\n",
    "    return basic_stats     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These results are for the __ nsubj __ dependency relation tag.\n"
     ]
    }
   ],
   "source": [
    "# load the pickle file for the dependency relation you are interested in. \n",
    "data = pickleloader(\"./UDdeprel_data/nsubj_UDdeprel_pkl\")\n",
    "\n",
    "# run the basic_statistics function on the data\n",
    "result = basic_statistics(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['head_dep_pos', 'deprel_subtags', 'dep_subdep_pos', 'deprel_on_head'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deprel_subtag</th>\n",
       "      <th>count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NOUN_det_DET</td>\n",
       "      <td>8286</td>\n",
       "      <td>29.708508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOUN_amod_ADJ</td>\n",
       "      <td>3631</td>\n",
       "      <td>13.018536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOUN_nmod_NOUN</td>\n",
       "      <td>2392</td>\n",
       "      <td>8.576243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOUN_compound_NOUN</td>\n",
       "      <td>1327</td>\n",
       "      <td>4.757807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOUN_nmod_PRON</td>\n",
       "      <td>1324</td>\n",
       "      <td>4.747051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NOUN_acl_VERB</td>\n",
       "      <td>1234</td>\n",
       "      <td>4.424366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PROPN_flat_PROPN</td>\n",
       "      <td>852</td>\n",
       "      <td>3.054749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NOUN_nmod_PROPN</td>\n",
       "      <td>798</td>\n",
       "      <td>2.861138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NOUN_conj_NOUN</td>\n",
       "      <td>752</td>\n",
       "      <td>2.696210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NOUN_punct_PUNCT</td>\n",
       "      <td>732</td>\n",
       "      <td>2.624503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PROPN_det_DET</td>\n",
       "      <td>624</td>\n",
       "      <td>2.237281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PROPN_compound_PROPN</td>\n",
       "      <td>504</td>\n",
       "      <td>1.807035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NOUN_nmod_DET</td>\n",
       "      <td>403</td>\n",
       "      <td>1.444911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NOUN_nummod_NUM</td>\n",
       "      <td>376</td>\n",
       "      <td>1.348105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NOUN_compound_PROPN</td>\n",
       "      <td>344</td>\n",
       "      <td>1.233373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PROPN_punct_PUNCT</td>\n",
       "      <td>324</td>\n",
       "      <td>1.161665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PROPN_conj_PROPN</td>\n",
       "      <td>247</td>\n",
       "      <td>0.885590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NOUN_advmod_ADV</td>\n",
       "      <td>155</td>\n",
       "      <td>0.555735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NOUN_amod_VERB</td>\n",
       "      <td>147</td>\n",
       "      <td>0.527052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PROPN_compound_NOUN</td>\n",
       "      <td>147</td>\n",
       "      <td>0.527052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PROPN_nmod_PROPN</td>\n",
       "      <td>138</td>\n",
       "      <td>0.494783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NOUN_appos_PROPN</td>\n",
       "      <td>131</td>\n",
       "      <td>0.469686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PRON_acl_VERB</td>\n",
       "      <td>123</td>\n",
       "      <td>0.441002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PROPN_amod_ADJ</td>\n",
       "      <td>123</td>\n",
       "      <td>0.441002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PROPN_appos_NOUN</td>\n",
       "      <td>117</td>\n",
       "      <td>0.419490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>PRON_det_DET</td>\n",
       "      <td>94</td>\n",
       "      <td>0.337026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NOUN_appos_NOUN</td>\n",
       "      <td>90</td>\n",
       "      <td>0.322685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>PRON_nmod_NOUN</td>\n",
       "      <td>89</td>\n",
       "      <td>0.319099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NUM_nmod_NOUN</td>\n",
       "      <td>73</td>\n",
       "      <td>0.261733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>PRON_punct_PUNCT</td>\n",
       "      <td>72</td>\n",
       "      <td>0.258148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>INTJ_det_DET</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>ADJ_appos_NOUN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>NOUN_advmod_PRON</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>PROPN_parataxis_PROPN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>NOUN_flat_NOUN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>X_nmod_X</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>NOUN_amod_SCONJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>NUM_advmod_ADP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>PRON_conj_ADJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>DET_acl_ADJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>PRON_nmod_ADV</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>PRON_obl_NOUN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>NOUN_parataxis_ADV</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>SYM_amod_ADJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>SYM_nummod_ADJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>NOUN_cc_ADV</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>NOUN_advmod_NUM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>PROPN_parataxis_ADV</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>NOUN_discourse_INTJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>PROPN_dislocated_NOUN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>NUM_nmod_ADV</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>NOUN_compound_PRON</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>ADJ_ccomp_VERB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>ADJ_conj_PRON</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>VERB_nmod_PRON</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>NOUN_expl_ADV</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>PRON_nmod_DET</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>ADV_nmod_PRON</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>DET_advcl_VERB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>DET_nmod_ADJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>356 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             deprel_subtag  count  percentage\n",
       "0             NOUN_det_DET   8286   29.708508\n",
       "1            NOUN_amod_ADJ   3631   13.018536\n",
       "2           NOUN_nmod_NOUN   2392    8.576243\n",
       "3       NOUN_compound_NOUN   1327    4.757807\n",
       "4           NOUN_nmod_PRON   1324    4.747051\n",
       "5            NOUN_acl_VERB   1234    4.424366\n",
       "6         PROPN_flat_PROPN    852    3.054749\n",
       "7          NOUN_nmod_PROPN    798    2.861138\n",
       "8           NOUN_conj_NOUN    752    2.696210\n",
       "9         NOUN_punct_PUNCT    732    2.624503\n",
       "10           PROPN_det_DET    624    2.237281\n",
       "11    PROPN_compound_PROPN    504    1.807035\n",
       "12           NOUN_nmod_DET    403    1.444911\n",
       "13         NOUN_nummod_NUM    376    1.348105\n",
       "14     NOUN_compound_PROPN    344    1.233373\n",
       "15       PROPN_punct_PUNCT    324    1.161665\n",
       "16        PROPN_conj_PROPN    247    0.885590\n",
       "17         NOUN_advmod_ADV    155    0.555735\n",
       "18          NOUN_amod_VERB    147    0.527052\n",
       "19     PROPN_compound_NOUN    147    0.527052\n",
       "20        PROPN_nmod_PROPN    138    0.494783\n",
       "21        NOUN_appos_PROPN    131    0.469686\n",
       "22           PRON_acl_VERB    123    0.441002\n",
       "23          PROPN_amod_ADJ    123    0.441002\n",
       "24        PROPN_appos_NOUN    117    0.419490\n",
       "25            PRON_det_DET     94    0.337026\n",
       "26         NOUN_appos_NOUN     90    0.322685\n",
       "27          PRON_nmod_NOUN     89    0.319099\n",
       "28           NUM_nmod_NOUN     73    0.261733\n",
       "29        PRON_punct_PUNCT     72    0.258148\n",
       "..                     ...    ...         ...\n",
       "326           INTJ_det_DET      1    0.003585\n",
       "327         ADJ_appos_NOUN      1    0.003585\n",
       "328       NOUN_advmod_PRON      1    0.003585\n",
       "329  PROPN_parataxis_PROPN      1    0.003585\n",
       "330         NOUN_flat_NOUN      1    0.003585\n",
       "331               X_nmod_X      1    0.003585\n",
       "332        NOUN_amod_SCONJ      1    0.003585\n",
       "333         NUM_advmod_ADP      1    0.003585\n",
       "334          PRON_conj_ADJ      1    0.003585\n",
       "335            DET_acl_ADJ      1    0.003585\n",
       "336          PRON_nmod_ADV      1    0.003585\n",
       "337          PRON_obl_NOUN      1    0.003585\n",
       "338     NOUN_parataxis_ADV      1    0.003585\n",
       "339           SYM_amod_ADJ      1    0.003585\n",
       "340         SYM_nummod_ADJ      1    0.003585\n",
       "341            NOUN_cc_ADV      1    0.003585\n",
       "342        NOUN_advmod_NUM      1    0.003585\n",
       "343    PROPN_parataxis_ADV      1    0.003585\n",
       "344    NOUN_discourse_INTJ      1    0.003585\n",
       "345  PROPN_dislocated_NOUN      1    0.003585\n",
       "346           NUM_nmod_ADV      1    0.003585\n",
       "347     NOUN_compound_PRON      1    0.003585\n",
       "348         ADJ_ccomp_VERB      1    0.003585\n",
       "349          ADJ_conj_PRON      1    0.003585\n",
       "350         VERB_nmod_PRON      1    0.003585\n",
       "351          NOUN_expl_ADV      1    0.003585\n",
       "352          PRON_nmod_DET      1    0.003585\n",
       "353          ADV_nmod_PRON      1    0.003585\n",
       "354         DET_advcl_VERB      1    0.003585\n",
       "355           DET_nmod_ADJ      1    0.003585\n",
       "\n",
       "[356 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see available information, stored in keys of a dictionary in the results \n",
    "print(result.keys())\n",
    "\n",
    "# access the results via the keys\n",
    "result[\"dep_subdep_pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
