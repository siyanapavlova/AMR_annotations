{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UD English corpora analysis. \n",
    "## Objectives\n",
    "\n",
    "1. For each dependency relation in UD, identify every single occurence of it, in every available English UD corpora. \n",
    "2. Obtain minimal information on each of these occurence (basic UDdeprel, Head and Dep classes, a helper function to \n",
    "to iterate through the list of deprels to extract info on). \n",
    "2a. do we care about the sub tags such nsubj:pass? Yes, it helps us identify possible subcases to analyse. \n",
    "3. Aggregate the data for these occurences at the dependency relation level to obtain insights (pairplots, correlation+heatmap for each POS-TAG pair)\n",
    "4. Where necessary, expand the analysis for a particular dependency relation (e.g. look at all Dep nodes and/or grandparent nodes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notes on corpora issues identified along the process__\n",
    "1. inconsistency in data captured in each UD English corpora. For instance, the ESL corpora does not capture lemma information. \n",
    "2. slight variations in relations numbering. EWT uses sub-decimals, \n",
    "3. ESL captures contractions, e.g. \"cannot\" is expanded into \"can\" and \"not\", and the dependency relations are tagged accordingly. \"cannot\" is captured too, but given a token index that is a combination of its units, e.g. 10-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes on usage of this notebook \n",
    "1. To get analysis done, run all the cells in Section 0, and then you can jump to Section 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preliminaries - load dependencies, helper functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu # https://github.com/EmilStenstrom/conllu\n",
    "import pandas as pd\n",
    "import glob\n",
    "from conllu import parse\n",
    "from conllu import parse_incr\n",
    "import re\n",
    "import _pickle as pickle\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load pickle file \n",
    "def pickleloader(filename):\n",
    "    # # open the file for writing\n",
    "    fileObject = open(filename,'rb') \n",
    "\n",
    "    # load the object from the file into var univ_processed_train\n",
    "    return pickle.load(fileObject,  encoding=\"latin1\")  \n",
    "    #latin1 here, to bypass python2 to 3 pickle problem\n",
    "\n",
    "    # here we close the fileObject\n",
    "    fileObject.close()\n",
    "    \n",
    "# helper function to create a file and store the data in the file \n",
    "def picklemaker(filename, objectname): \n",
    "    # open the file for writing\n",
    "    fileObject = open(filename,'wb') \n",
    "\n",
    "    # this writes the object a to the file named 'testfile'\n",
    "    pickle.dump(objectname,fileObject)   \n",
    "\n",
    "    # here we close the fileObject\n",
    "    fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UDdeprel:\n",
    "    '''\n",
    "    basic class of objects for capturing tokens with a particular UD dependency relationship, its form and POS tag. \n",
    "    This is the dependent node on which the deprel lands on. The class also includes functions as well as functions \n",
    "    to find its head and dependents. \n",
    "    --  -- \n",
    "    Note that self.head is a UDdeprel object. \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, sentence_num:int, self_id:int, self_form:str, deprel:str, \n",
    "                 self_pos:str, head_id: int, deprel_sub=None, head=None, \n",
    "                 deps=None, siblings=None):\n",
    "        self.form = self_form             # using form instead of lemma because some UD Eng corpora (e.g. CESL) don't have lemma information \n",
    "        self.self_id = self_id\n",
    "        self.deprel = deprel\n",
    "        self.deprel_sub = deprel_sub\n",
    "        self.self_pos = self_pos\n",
    "        self.sentence_num = sentence_num\n",
    "        self.head_id = head_id\n",
    "        self.head = head\n",
    "        self.deps = deps\n",
    "        self.siblings = siblings\n",
    "    \n",
    "    def __str__(self):\n",
    "        print(\"Form:\", self.form, \" DepRel:\", self.deprel, \" DepRel_Sub:\", self.deprel_sub,\n",
    "             \" PoS:\", self.self_pos, \" SentNum:\", self.sentence_num)\n",
    "    \n",
    "    def find_head(self, UD_sent_dictionary):\n",
    "        sentence = UD_sent_dictionary[self.sentence_num][0] # [0] is required by default for every sentence to access its tokesn\n",
    "       \n",
    "        H_sentence_num = self.sentence_num\n",
    "        H_self_id = self.head_id                        # this is the id number in the conllu format \n",
    "                                                        # and not the same as the dictionary's\n",
    "                                                        # which is indexed from 0\n",
    "                \n",
    "        # find the token having the H_self_id in its \"id\" key\n",
    "        # this approach ensures accuracy, since token id numbers are not aligned \n",
    "        # with index numbers in sentences (e.g. there are no root tokens), certain \n",
    "        # UD English corpora (e.g. EWT) use sub index numbers (e.g. 11 and 11.1) for \n",
    "        # labeling the tokens in the corpus. \n",
    "        H_token =  [token for token in sentence if token[\"id\"]==H_self_id][0]\n",
    "        \n",
    "        H_self_form = H_token[\"form\"]\n",
    "        H_head_id = H_token[\"head\"]\n",
    "        H_deprel = re.findall(r\"^[A-Za-z]+[^:]\", H_token[\"deprel\"])[0]  \n",
    "        H_deprel_sub = H_token[\"deprel\"].lstrip(H_deprel+\":\")\n",
    "        if H_deprel_sub == \"\":\n",
    "            H_deprel_sub = None\n",
    "        H_self_pos = H_token[\"upostag\"]\n",
    "\n",
    "        __head = Head(sentence_num = H_sentence_num,\n",
    "                        self_id = H_self_id, \n",
    "                        self_form = H_self_form,\n",
    "                        head_id = H_head_id,\n",
    "                        deprel = H_deprel,\n",
    "                        self_pos = H_self_pos,\n",
    "                        deprel_sub = H_deprel_sub)\n",
    "                        \n",
    "                        # to prevent confusion, self_id = H_self_id+1 is required to return the token ID number to \n",
    "                        # one in the conllu data\n",
    "        self.head = __head\n",
    "        \n",
    "    def find_deps(self,UD_sent_dictionary):\n",
    "        dep_id = self.self_id # this is the index number of the primary UDdeprel token\n",
    "        sentence = UD_sent_dictionary[self.sentence_num][0] # [0] is required by default for every sentence to access its tokesn\n",
    "        \n",
    "        D_sentence_num = self.sentence_num\n",
    "        \n",
    "        D_tokens = [token for token in sentence if token[\"head\"]==dep_id]\n",
    "        # the conllu has a field to capture all the dependents of a particular token. \n",
    "        # however, not all UD Eng corpora (e.g. CESL) populate this field. Therefore, \n",
    "        # a seach of each token in a sentence has to be done here. \n",
    "        \n",
    "\n",
    "        \n",
    "        __deps = []\n",
    "        for D_token in D_tokens:\n",
    "            D_self_id = D_token[\"id\"]      \n",
    "            D_head_id = D_token[\"head\"]    # again, this is the index given in the conllu format\n",
    "            D_self_form = D_token[\"form\"],\n",
    "            D_head_id = D_token[\"head\"],\n",
    "            D_deprel = re.findall(r\"^[A-Za-z]+[^:]\", D_token[\"deprel\"])[0]\n",
    "            D_deprel_sub = D_token[\"deprel\"].lstrip(D_deprel+\":\")\n",
    "            if D_deprel_sub == \"\":\n",
    "                D_deprel_sub = None\n",
    "            D_self_pos = D_token[\"upostag\"]\n",
    "\n",
    "            # r\"^[A-Za-z]+[^:]\" for D_self_pos because some of the UD English corpora have the postag in all lowercase\n",
    "            # and some in all uppercase. \n",
    "\n",
    "\n",
    "            __dep = Dep(sentence_num = D_sentence_num,\n",
    "                            self_id = D_self_id,\n",
    "                            self_form = D_self_form,\n",
    "                            head_id = D_head_id,\n",
    "                            deprel = D_deprel,\n",
    "                            self_pos = D_self_pos,\n",
    "                            deprel_sub = D_deprel_sub)\n",
    "\n",
    "\n",
    "            __deps.append(__dep)\n",
    "        self.deps = __deps\n",
    "            \n",
    "    def find_siblings(self, UD_sent_dictionary):\n",
    "        head_id = self.head_id # conllu number of the head of the primary UDdeprel token\n",
    "        sentence = UD_sent_dictionary[self.sentence_num][0] # [0] is required by default for every sentence to access its tokesn\n",
    "        S_sentence_num = self.sentence_num\n",
    "        \n",
    "        # match only the tokens having the head_id, but exclude the one having self.self_id, to find only siblings\n",
    "        S_tokens = [token for token in sentence if token[\"head\"]==head_id and token[\"id\"] != self.self_id]\n",
    "        \n",
    "        __siblings = []\n",
    "        for S_token in S_tokens:\n",
    "            S_self_id = S_token[\"id\"]     \n",
    "            S_head_id = S_token[\"head\"]    # again, this is the index given in the conllu format\n",
    "            S_self_form = S_token[\"form\"],\n",
    "            S_head_id = S_token[\"head\"],\n",
    "            S_deprel = re.findall(r\"^[A-Za-z]+[^:]\", S_token[\"deprel\"])[0]\n",
    "            S_deprel_sub = S_token[\"deprel\"].lstrip(S_deprel+\":\")\n",
    "            if S_deprel_sub == \"\":\n",
    "                S_deprel_sub = None\n",
    "            S_self_pos = S_token[\"upostag\"]\n",
    "\n",
    "            __sibling = Sibling(sentence_num = S_sentence_num,\n",
    "                            self_id = S_self_id,\n",
    "                            self_form = S_self_form,\n",
    "                            head_id = S_head_id,\n",
    "                            deprel = S_deprel,\n",
    "                            self_pos = S_self_pos,\n",
    "                            deprel_sub = S_deprel_sub)\n",
    "\n",
    "\n",
    "            __siblings.append(__sibling)\n",
    "        self.siblings = __siblings\n",
    "\n",
    "class Head(UDdeprel):\n",
    "    '''\n",
    "    basic UDdeprel that inherits all the methods and attributes of the UDdeprel. \n",
    "    Amend or add to for the purpose of your analysis. \n",
    "    '''\n",
    "    pass\n",
    "    \n",
    "class Dep(UDdeprel): \n",
    "    '''\n",
    "    basic UDdeprel that inherits all the methods and attributes of the UDdeprel. \n",
    "    Amend or add to for the purpose of your analysis. \n",
    "    '''\n",
    "    pass\n",
    "\n",
    "class Sibling(UDdeprel):\n",
    "    '''\n",
    "    basic UDdeprel that inherits all the methods and attributes of the UDdeprel. \n",
    "    Intended for collecting information about a particular token's sibling tokens (i.e. \n",
    "    sharing the same Head node). Amend or add to for the purpose of your analysis. \n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Consolidating the datasets, extracting only the relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all the filenames for the English UD corporas \n",
    "conllu_filenames = glob.glob(\"./UD_eng_rawfiles/*.conllu\")\n",
    "conllu_filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  ./UD_eng_rawfiles/en_cesl-ud-dev.conllu  | total number of sentences:  500\n",
      "Processed:  ./UD_eng_rawfiles/en_cesl-ud-test.conllu  | total number of sentences:  1000\n",
      "Processed:  ./UD_eng_rawfiles/en_cesl-ud-train.conllu  | total number of sentences:  5124\n",
      "Processed:  ./UD_eng_rawfiles/en_ewt-ud-dev.conllu  | total number of sentences:  7126\n",
      "Processed:  ./UD_eng_rawfiles/en_ewt-ud-test.conllu  | total number of sentences:  9203\n",
      "Processed:  ./UD_eng_rawfiles/en_ewt-ud-train.conllu  | total number of sentences:  21746\n",
      "Processed:  ./UD_eng_rawfiles/en_gum-ud-dev.conllu  | total number of sentences:  22453\n",
      "Processed:  ./UD_eng_rawfiles/en_gum-ud-test.conllu  | total number of sentences:  23231\n",
      "Processed:  ./UD_eng_rawfiles/en_gum-ud-train.conllu  | total number of sentences:  26145\n",
      "Processed:  ./UD_eng_rawfiles/en_lines-ud-dev.conllu  | total number of sentences:  27057\n",
      "Processed:  ./UD_eng_rawfiles/en_lines-ud-test.conllu  | total number of sentences:  27971\n",
      "Processed:  ./UD_eng_rawfiles/en_lines-ud-train.conllu  | total number of sentences:  30709\n",
      "Processed:  ./UD_eng_rawfiles/en_partut-ud-dev.conllu  | total number of sentences:  30865\n",
      "Processed:  ./UD_eng_rawfiles/en_partut-ud-test.conllu  | total number of sentences:  31018\n",
      "Processed:  ./UD_eng_rawfiles/en_partut-ud-train.conllu  | total number of sentences:  32799\n",
      "Processed:  ./UD_eng_rawfiles/en_pud-ud-test.conllu  | total number of sentences:  33799\n"
     ]
    }
   ],
   "source": [
    "def conllu_metadata_stripper(filename:str, sentence_dict:dict): \n",
    "    '''\n",
    "    takes a file containing text in CONLLU format, filters for tokens (removing metadata), collects them into sentences, \n",
    "    adds them to the dictionary of sentences. \n",
    "    inputs | filename:str - name of the file to be processed ; sentence_dict:dict - dictionary, either empty\n",
    "    or containing CONLLU tokens in sentences (such as those already processed from other files with CONLLU data) \n",
    "    returns | sentence_dict:dict - containing the tokens collected from the CONLLU file being processed\n",
    "    '''\n",
    "    #open the file \n",
    "    data_file = open(filename,\"r\",encoding=\"utf-8\")\n",
    "    # get all the sentences... i.e. by collecting all the tokens and ignoring the metadata before every sentence\n",
    "    token_list = [token for token in data_file.readlines() if re.match(r\"([0-9]+\\.?[0-9]*\\s)\\S*\",  token)] \n",
    "    # close the file \n",
    "    data_file.close()\n",
    "    \n",
    "\n",
    "    # use a counter identify each sentence added to the dictionary\n",
    "    sent_counter = len(sentence_dict)+1\n",
    "    \n",
    "    \n",
    "    def token_adder(sent_counter,sentence_dict):\n",
    "        # use a counter which we will control the addition of tokens to each key in the dictionary\n",
    "        tok_counter = 0\n",
    "        \n",
    "        # initialise an empty list with the current dictionary key \n",
    "        sentence_dict[sent_counter] = []\n",
    "        \n",
    "        # try except to pass when list has been emptied\n",
    "        try: \n",
    "            # \"local\" while loop to add tokens only if they have numbers bigger than tok_counter\n",
    "            # i.e. once the index number for a token falls below the last, reset the while loop\n",
    "            while float(re.findall(r\"^[0-9]+\\.?[0-9]*\", token_list[0])[0]) > tok_counter:\n",
    "                tok_counter = float(re.findall(r\"^[0-9]+\\.?[0-9]*\", token_list[0])[0])\n",
    "                \n",
    "                # pop the first element from the token_list \n",
    "                sentence_dict[sent_counter].append(token_list.pop(0))\n",
    "                 \n",
    "                \n",
    "                # note on the regex above: \n",
    "                # there is corpora that index tokens with floating point numbers. e.g. a token can be 18\n",
    "                # and another related one can be 18.1. see sentence 17838 in the dict, which is from the \n",
    "                # EWT corpora. The regex captures either the full string containing a float, or the full string\n",
    "                # containing an int. The string is then converted to a float (instead of int) to avoid errors \n",
    "                # in token identification due to roundings.\n",
    "                \n",
    "            else: \n",
    "                tok_counter = 0\n",
    "                pass\n",
    "        except: \n",
    "            pass \n",
    "        \n",
    "        \n",
    "    # while loop to continue calling token_adder recursively until the list is empty     \n",
    "    while len(token_list)>0: \n",
    "        token_adder(sent_counter,sentence_dict)\n",
    "        sent_counter += 1\n",
    "\n",
    "\n",
    "    return sentence_dict\n",
    "\n",
    "UDen_sent_dict = {}\n",
    "for file in conllu_filenames: \n",
    "    \n",
    "    conllu_metadata_stripper(file, UDen_sent_dict)\n",
    "    print (\"Processed: \", file, \" | total number of sentences: \", len(UDen_sent_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the parse_incr method from the conllu package to parse the information for each token\n",
    "# the parsing results in the creation of a TokenList object which contains each of the token in \n",
    "# a sentence and seperates the conllu data into seperate keys\n",
    "# to access a specific token, <dictionary>[<sentence number>][0][<token number>]\n",
    "\n",
    "UDen_sent_parsedlist = {}\n",
    "for sentence in UDen_sent_dict:\n",
    "    UDen_sent_parsedlist[sentence] = []\n",
    "    for token in parse_incr(UDen_sent_dict[sentence]):\n",
    "        UDen_sent_parsedlist[sentence].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Pickler in progress\n",
    "picklemaker(\"./UDdeprel_data/UDen_sent_parsedlist\", UDen_sent_parsedlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Function to collect basic information about tokens with a particular UD relation landing on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "UD_deprel_list = [\"nsubj\", \"obj\", \"iobj\", \"csubj\", \"ccomp\", \"xcomp\", \"obl\", \n",
    "                  \"vocative\", \"expl\",\"dislocated\",\"advcl\",\"advmod\", \"discourse\",\n",
    "                  \"aux\",\"cop\", \"mark\",\"nmod\",\"appos\", \"nummod\", \"acl\", \"amod\", \"det\", \n",
    "                  \"clf\", \"case\", \"conj\", \"cc\", \"fixed\", \"flat\", \"compound\", \"list\",\n",
    "                  \"parataxis\", \"orphan\", \"goeswith\", \"reparandum\", \"punct\", \"dep\"]\n",
    "\n",
    "# note: we are not including the root dependency relation in our analysis here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprelfinder(deprel, UD_sent_dictionary):\n",
    "    '''\n",
    "    takes a dictionary containing the sentences (and tokens) of one or more UD english corpora, searches for \n",
    "    tokens that have this dependency relation (these tokens would be the dependent nodes in a dep tree).  \n",
    "    This function aggregates data in a manner that would allow a more fine-grained analysis. A token's dependency\n",
    "    relation is set at main tag level, with subtag information captured at the deprel_sub level.  \n",
    "    _________________\n",
    "   \n",
    "    input | deprel:str, the dependency relation of interest; UD_sent_dictionary: dict, a dictionary containing \n",
    "    UD sentences as parsed from the conllu format with the conllu python package \n",
    "    (see https://github.com/EmilStenstrom/conllu); output: str, \"pickle\" by default, or \"csv\", the format to \n",
    "    save the dependency relation search results in. \n",
    "    \n",
    "    output | a pickle containing a list of UDdeprel objects containing information about all the tokens in the \n",
    "    UD_sent_dictionary, having the dependency relation of interest. The information captured by this function is \n",
    "    sentence_num, self_form, deprel, deprel_sub, self_pos. \n",
    "    '''\n",
    "    failures= [] \n",
    "    \n",
    "    __UDdeprel_list = []\n",
    "    \n",
    "    # loop through every sentence in the dictionary\n",
    "    for sentence in UD_sent_dictionary: #sentence is an int from 1 to n, where n is the size of the dictionary. \n",
    "        \n",
    "        # go into every token for each sentence and check for tokens that have the deprel we are interested in. \n",
    "        for token in UD_sent_dictionary[sentence][0]:\n",
    "            \n",
    "            #use regex to match deprels because there are some that have subtags such as nsubj:pass\n",
    "            if re.match(\"^\"+deprel, token[\"deprel\"]):\n",
    "                \n",
    "                sentence_num=sentence\n",
    "                self_id = token[\"id\"]   # this is the id number in the conllu format and not the same as the dictionary's\n",
    "                                        # which is indexed from 0\n",
    "                self_form=token[\"form\"]\n",
    "                head_id = token[\"head\"]\n",
    "                deprel = re.findall(r\"^[A-Za-z]+[^:]\", deprel)[0]  \n",
    "                deprel_sub = token[\"deprel\"].lstrip(deprel+\":\") #use lstrip to remove the main deprel+: tag\n",
    "                \n",
    "                if deprel_sub == \"\":\n",
    "                    deprel_sub=None\n",
    "                self_pos=token[\"upostag\"]\n",
    "                \n",
    "                __UDdeprel = UDdeprel(sentence_num = sentence_num,\n",
    "                                      self_id = self_id,\n",
    "                                      self_form = self_form, \n",
    "                                      head_id = head_id,\n",
    "                                      deprel = deprel, \n",
    "                                      self_pos = self_pos,\n",
    "                                      deprel_sub = deprel_sub)\n",
    "                \n",
    "                try:\n",
    "                    __UDdeprel.find_head(UDen_sent_parsedlist)\n",
    "                    __UDdeprel.find_deps(UDen_sent_parsedlist)\n",
    "                    __UDdeprel.find_siblings(UDen_sent_parsedlist)\n",
    "                except: \n",
    "                    failures.append(__UDdeprel)\n",
    "\n",
    "                __UDdeprel_list.append(__UDdeprel)\n",
    "    \n",
    "    # let's pickle the results\n",
    "    filename = \"./UDdeprel_data/\"+deprel+\"_UDdeprel_pkl\"\n",
    "    picklemaker(filename, __UDdeprel_list)\n",
    "    \n",
    "    return failures\n",
    "\n",
    "\n",
    "# run the function and start collecting the token info and storing into pickles. if a pickle file with the same \n",
    "# filename already exists, the picklemaker function opens the file and dumps the object in it. it overwrites the \n",
    "# existing content. if you want to save the version of the previous pickle file, adjust the filename variable in the \n",
    "# function. \n",
    "failures__ = []\n",
    "for deprel in UD_deprel_list: \n",
    "    failures__.extend(deprelfinder(deprel, UDen_sent_parsedlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# failure check on find_head and find_deps methods. \n",
    "len(failures__)\n",
    "# no failures in acquiring information. next step is some basic unit tests to check accuracy of acquired information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes \n",
    "1. Every UD dependency relation has a pickle file now. The start of each pickle filename identifies the dependency relation tag that it is for. \n",
    "2. In each pickle file is the list of tokens having that particular dependency relation. The tokens are stored as objects of the UDdeprel class from above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analysis and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_statistics(UDdeprel_list):\n",
    "    '''\n",
    "    Purpose of this function is to provide basic statistics on the phenomonena in one (or more) Universal \n",
    "    Dependencies-annotated corpora.\n",
    "    \n",
    "    returns | basic_stats which provides information about the set of UDdeprel objects for a particular UD dependency \n",
    "    relation tag. \n",
    "    \n",
    "    1a. head_dep_pos    | identifies the prototypical head-dep POS pair. returns a table with the counts and percentage \n",
    "                            of a particular POS-pair for the dependency relation tag of interest. \n",
    "    1b. deprel_subtags  | identifies the presence (or absence) of subtags in the corpora for the dependency relation tag. \n",
    "                            returns a table with count and percentage figures of the subtags. \n",
    "    2.  dep_subdep_pos  | identifies the prototypical dep-subdep POS pair and their dependency relation. returns a table \n",
    "                            with the counts and percentage of the occurrence across the corpora. \n",
    "    3a. deprel_on_head     | identifies the prototypical dependency relation that lands on the head. \n",
    "    3b. deprel_out_head    | identifies the most common dependency relation that exits the head\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # =+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+=    \n",
    "    # 1a. governor-dependent POS analysis \n",
    "    __head_dep_pos = []\n",
    "    for dependent in UDdeprel_list: \n",
    "        head_POS = dependent.head.self_pos\n",
    "        dep_POS = dependent.self_pos\n",
    "        dep_deprel = dependent.deprel\n",
    "        __head_dep_pos.append(head_POS + \"_{}_\".format(dep_deprel) + dep_POS)\n",
    "    \n",
    "    __head_dep_pos_counter = collections.Counter(x for x in __head_dep_pos if x)\n",
    "    \n",
    "    df_1a = pd.DataFrame.from_dict(__head_dep_pos_counter, orient='index').reset_index()\n",
    "    df_1a.columns = [\"head_dep_pos\", \"count\"]\n",
    "    df_1a.sort_values(by=\"count\", ascending=False, inplace=True)\n",
    "    df_1a.reset_index(drop=True,inplace=True)\n",
    "    df_1a[\"percentage\"] = df_1a[\"count\"].copy()\n",
    "    df_1a[\"percentage\"] =  df_1a[\"percentage\"] / df_1a[\"count\"].sum()*100\n",
    "    \n",
    "    basic_stats = {\"head_dep_pos\": df_1a}\n",
    "    \n",
    "    # =+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+=\n",
    "    # 1b. presence of subtags, and frequency + percentage \n",
    "    \n",
    "    __deprel_subtags = [\"none\" if dependent.deprel_sub==None \n",
    "                        else dependent.deprel_sub for dependent in UDdeprel_list]\n",
    "    \n",
    "    __deprel_subtags_counter = collections.Counter(x for x in __deprel_subtags if x)\n",
    "\n",
    "    df_1b = pd.DataFrame.from_dict(__deprel_subtags_counter, orient='index').reset_index()\n",
    "    df_1b.columns = [\"deprel_subtag\", \"count\"]\n",
    "    df_1b.sort_values(by=\"count\", ascending=False, inplace=True)\n",
    "    df_1b.reset_index(drop=True,inplace=True)\n",
    "    df_1b[\"percentage\"] = df_1b[\"count\"].copy()\n",
    "    df_1b[\"percentage\"] =  df_1b[\"percentage\"] / df_1b[\"count\"].sum()*100   \n",
    "    \n",
    "    basic_stats[\"deprel_subtags\"] = df_1b\n",
    "\n",
    "    # =+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+=\n",
    "    # 2. Dependent analysis: gives the most prototypical dependent-subdependent relationship (i.e. their POS \n",
    "    # and deprel). \n",
    "    \n",
    "    __dep_subdep_pos = []\n",
    "    for dependent in UDdeprel_list:\n",
    "        dep_pos = dependent.self_pos\n",
    "        __subdeps = dependent.deps\n",
    "        for __subdep in __subdeps:\n",
    "            subdep_pos = __subdep.self_pos\n",
    "            subdep_deprel = __subdep.deprel\n",
    "            __dep_subdep_pos.append(dep_pos + \"_{}_\".format(subdep_deprel) + subdep_pos)\n",
    "    \n",
    "    __dep_subdep_pos_counter = collections.Counter(x for x in __dep_subdep_pos if x)\n",
    "\n",
    "    df_2 = pd.DataFrame.from_dict(__dep_subdep_pos_counter, orient='index').reset_index()\n",
    "    df_2.columns = [\"deprel_subtag\", \"count\"]\n",
    "    df_2.sort_values(by=\"count\", ascending=False, inplace=True)\n",
    "    df_2.reset_index(drop=True,inplace=True)\n",
    "    df_2[\"percentage\"] = df_2[\"count\"].copy()\n",
    "    df_2[\"percentage\"] =  df_2[\"percentage\"] / df_2[\"count\"].sum()*100   \n",
    "    \n",
    "    basic_stats[\"dep_subdep_pos\"] = df_2\n",
    "    \n",
    "    # =+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+=\n",
    "    # 3a. Head analysis: most common deprel main tag that lands on the head of the UDdeprel\n",
    "    \n",
    "    __deprel_on_head = [\"none\" if dependent.head.deprel==None \n",
    "                        else dependent.head.deprel for dependent in UDdeprel_list]\n",
    "    \n",
    "    __deprel_on_head_counter = collections.Counter(x for x in __deprel_on_head if x)\n",
    "\n",
    "    df_3a = pd.DataFrame.from_dict(__deprel_on_head_counter, orient='index').reset_index()\n",
    "    df_3a.columns = [\"deprel_on_head\", \"count\"]\n",
    "    df_3a.sort_values(by=\"count\", ascending=False, inplace=True)\n",
    "    df_3a.reset_index(drop=True,inplace=True)\n",
    "    df_3a[\"percentage\"] = df_3a[\"count\"].copy()\n",
    "    df_3a[\"percentage\"] =  df_3a[\"percentage\"]/df_3a[\"count\"].sum()*100   \n",
    "    \n",
    "    basic_stats[\"deprel_on_head\"] = df_3a\n",
    "    \n",
    "    # =+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+=\n",
    "    # 3b. Head analysis: most common dep_rels that exits the head of the UDdeprel, (ii) most common sets of \n",
    "    # dep_rels that exit the head, (iii) most common subsets of dep_rels that exit the head [using intersection]... \n",
    "    # filter off heads with single dep_rels [already inferable from (i)]\n",
    "    \n",
    "    # \"mine\" the children nodes of the governor \n",
    "    [data[i].head.find_deps(UDen_sent_parsedlist) for i in range(len(data))]\n",
    "    \n",
    "    __deprel_out_head = [\"none\" if dependent.head.deps == None\n",
    "                        else dep_of_gov.deprel for dependent in data if dependent.head.deps !=None \n",
    "                         for dep_of_gov in dependent.head.deps]\n",
    "    \n",
    "    __deprel_out_head_counter = collections.Counter(x for x in __deprel_out_head if x)\n",
    "    \n",
    "    df_3b = pd.DataFrame.from_dict(__deprel_out_head_counter, orient='index').reset_index()\n",
    "    df_3b.columns = [\"deprel_out_head\", \"count\"]\n",
    "    df_3b.sort_values(by=\"count\", ascending=False, inplace=True)\n",
    "    df_3b.reset_index(drop=True,inplace=True)\n",
    "    df_3b[\"percentage\"] = df_3b[\"count\"].copy()\n",
    "    df_3b[\"percentage\"] =  df_3b[\"percentage\"]/df_3b[\"count\"].sum()*100   \n",
    "    \n",
    "    basic_stats[\"deprel_out_head\"] = df_3b\n",
    "    \n",
    "    # =+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+==+=+=+=\n",
    "    # 5. heatmap of the head data and the dependent data. e.g. identify correlation across the dataset.\n",
    "    \n",
    "        # work-in-progress\n",
    "    print(\"These results are for the __ {} __ dependency relation tag.\".format(UDdeprel_list[0].deprel))\n",
    "    return basic_stats     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These results are for the __ obj __ dependency relation tag.\n"
     ]
    }
   ],
   "source": [
    "# load the pickle file for the dependency relation you are interested in. \n",
    "data = pickleloader(\"./UDdeprel_data/obj_UDdeprel_pkl\")\n",
    "\n",
    "# run the basic_statistics function on the data\n",
    "result = basic_statistics(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['head_dep_pos', 'deprel_subtags', 'dep_subdep_pos', 'deprel_on_head', 'deprel_out_head'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deprel_out_head</th>\n",
       "      <th>count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>obj</td>\n",
       "      <td>28608</td>\n",
       "      <td>25.877415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nsubj</td>\n",
       "      <td>15925</td>\n",
       "      <td>14.404986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>punct</td>\n",
       "      <td>15307</td>\n",
       "      <td>13.845973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mark</td>\n",
       "      <td>9676</td>\n",
       "      <td>8.752442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>obl</td>\n",
       "      <td>8883</td>\n",
       "      <td>8.035133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aux</td>\n",
       "      <td>7487</td>\n",
       "      <td>6.772379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>advmod</td>\n",
       "      <td>7400</td>\n",
       "      <td>6.693683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>conj</td>\n",
       "      <td>3789</td>\n",
       "      <td>3.427346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>advcl</td>\n",
       "      <td>3771</td>\n",
       "      <td>3.411064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cc</td>\n",
       "      <td>3644</td>\n",
       "      <td>3.296186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>xcomp</td>\n",
       "      <td>1968</td>\n",
       "      <td>1.780158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>compound</td>\n",
       "      <td>862</td>\n",
       "      <td>0.779724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>parataxis</td>\n",
       "      <td>804</td>\n",
       "      <td>0.727260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>iobj</td>\n",
       "      <td>753</td>\n",
       "      <td>0.681127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>discourse</td>\n",
       "      <td>465</td>\n",
       "      <td>0.420617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ccomp</td>\n",
       "      <td>382</td>\n",
       "      <td>0.345539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>csubj</td>\n",
       "      <td>138</td>\n",
       "      <td>0.124828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cop</td>\n",
       "      <td>128</td>\n",
       "      <td>0.115783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>vocative</td>\n",
       "      <td>100</td>\n",
       "      <td>0.090455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>expl</td>\n",
       "      <td>88</td>\n",
       "      <td>0.079601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nummod</td>\n",
       "      <td>63</td>\n",
       "      <td>0.056987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>dep</td>\n",
       "      <td>55</td>\n",
       "      <td>0.049750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>appos</td>\n",
       "      <td>49</td>\n",
       "      <td>0.044323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>case</td>\n",
       "      <td>39</td>\n",
       "      <td>0.035278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>acl</td>\n",
       "      <td>34</td>\n",
       "      <td>0.030755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>nmod</td>\n",
       "      <td>33</td>\n",
       "      <td>0.029850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>amod</td>\n",
       "      <td>31</td>\n",
       "      <td>0.028041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>det</td>\n",
       "      <td>23</td>\n",
       "      <td>0.020805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>dislocated</td>\n",
       "      <td>23</td>\n",
       "      <td>0.020805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>reparandum</td>\n",
       "      <td>13</td>\n",
       "      <td>0.011759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>list</td>\n",
       "      <td>6</td>\n",
       "      <td>0.005427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>goeswith</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>orphan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>flat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   deprel_out_head  count  percentage\n",
       "0              obj  28608   25.877415\n",
       "1            nsubj  15925   14.404986\n",
       "2            punct  15307   13.845973\n",
       "3             mark   9676    8.752442\n",
       "4              obl   8883    8.035133\n",
       "5              aux   7487    6.772379\n",
       "6           advmod   7400    6.693683\n",
       "7             conj   3789    3.427346\n",
       "8            advcl   3771    3.411064\n",
       "9               cc   3644    3.296186\n",
       "10           xcomp   1968    1.780158\n",
       "11        compound    862    0.779724\n",
       "12       parataxis    804    0.727260\n",
       "13            iobj    753    0.681127\n",
       "14       discourse    465    0.420617\n",
       "15           ccomp    382    0.345539\n",
       "16           csubj    138    0.124828\n",
       "17             cop    128    0.115783\n",
       "18        vocative    100    0.090455\n",
       "19            expl     88    0.079601\n",
       "20          nummod     63    0.056987\n",
       "21             dep     55    0.049750\n",
       "22           appos     49    0.044323\n",
       "23            case     39    0.035278\n",
       "24             acl     34    0.030755\n",
       "25            nmod     33    0.029850\n",
       "26            amod     31    0.028041\n",
       "27             det     23    0.020805\n",
       "28      dislocated     23    0.020805\n",
       "29      reparandum     13    0.011759\n",
       "30            list      6    0.005427\n",
       "31        goeswith      3    0.002714\n",
       "32          orphan      1    0.000905\n",
       "33            flat      1    0.000905"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see available information, stored in keys of a dictionary in the results \n",
    "print(result.keys())\n",
    "\n",
    "# access the results via the keys\n",
    "result[\"deprel_out_head\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"test\" in [\"test\", 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
